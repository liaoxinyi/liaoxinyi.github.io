---
layout:     post
title:      "可不敢吹自己会Redis-02"
subtitle:   "为什么Redis这么快？Redis的分布式锁怎么玩儿？zookeeper的分布式锁"
date:       2021-04-03
author:     "ThreeJin"
header-mask: 0.5
catalog: true
header-img: "https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-bk.png"
tags:
    - Java
    - Redis
    - Zookeeper

---
> 资料来源于网络上各位前辈（闪客sun）

### 前言
大家都知道Redis即使是单线程也能在高并发的场景下表现不俗，也都知道底层用的是I/O多路复用，但是真正知道什么是I/O多路复用吗？为什么会逐渐发展处这样的技术呢？这些都是需要自己去认真思考的，知其然也要知其所以然。
### 为什么Redis这么快？
##### 阻塞IO
最开始的时候，网络I/O采用的是阻塞IO，就像下面的伪代码那样：
```java
listenfd = socket();   // 打开一个网络通信端口
bind(listenfd);        // 绑定
listen(listenfd);      // 监听
while(1) {
  connfd = accept(listenfd);  // 阻塞建立连接
  int n = read(connfd, buf);  // 阻塞读数据
  doSomeThing(buf);  // 利用读到的数据做些什么
  close(connfd);     // 关闭连接，循环等待下一个连接
}
```

动图展现出来就是下面这样：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-01.gif)  
<center>阻塞IO示意图</center>  
可以看到，服务端的线程阻塞在了两个地方，一个是 accept 函数，一个是 read 函数。其中read 函数的底层会阻塞在下面两个阶段：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-02.gif)  
<center>read函数实现过程</center>  
总结一下，传统的阻塞IO大致流程如下：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-01.jpg)  
<center>传统的阻塞IO过程</center>  
问题的根本在于：  
**<font color=red>如果这个连接的客户端一直不发数据，那么服务端线程将会一直阻塞在 read 函数上不返回，也无法接受其他客户端连接</font>**

##### 非阻塞IO
既然问题在于read函数上，那么能不能改造一下呢？换做是我，肯定会想到每次都创建一个新的进程或线程，去调用 read 函数，并做业务处理。但是，这不叫非阻塞 IO，只不过用了多线程的手段使得主线程没有卡在 read 函数上不往下走罢了。操作系统为我们提供的 read 函数仍然是阻塞的

所以真正的非阻塞 IO，不能是通过我们用户层的小把戏，而是要**恳请操作系统为我们提供一个非阻塞的 read 函数**，这个 read 函数的效果是，如果没有数据到达时（到达网卡并拷贝到了内核缓冲区），立刻返回一个错误值（-1），而不是阻塞地等待

这样，就需要用户线程循环调用 read，直到返回值不为 -1，再开始处理业务，这种方法结合上层应用的多线程就能比较好的解决传统阻塞IO的问题了（注意是“比较好”，自然问题还是有的），动图表示如下：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-03.gif)  
<center>改进之后的read函数</center>  
其中有个细节：  
非阻塞的 read，指的是在数据到达前，即数据还未到达网卡，或者到达网卡但还没有拷贝到内核缓冲区之前，这个阶段是非阻塞的。**当数据已到达内核缓冲区，此时调用 read 函数仍然是阻塞的，需要等待数据从内核缓冲区拷贝到用户缓冲区，才能返回**

##### IO多路复用-select
上面的非阻塞IO+多线程虽然解决了新连接不被阻塞的问题，但是带来了新的问题：**为每个客户端创建一个线程，服务器端的线程资源很容易被耗光**  
所以，IO多路复用应运而生，最开始IO多路复用的原理如下：  
可以每 accept 一个客户端连接后，将这个文件描述符（connfd）放到一个数组里，然后弄一个新的线程去不断遍历这个数组，调用每一个元素的非阻塞 read 方法，这样就**成功用一个线程处理了多个客户端连接**  
这个时候就有个疑问了：每次都要去调用read函数，从上面也知道read函数涉及到了用户态和内核态的切换，那岂不是有很多次多于的切换开销呢？有没有办法：系统提供给我们一个有这样效果的函数，**将一批文件描述符通过一次系统调用传给内核，由内核层去遍历，才能真正解决这个问题**  
所以，select函数就来了：  
select 是操作系统提供的系统调用函数，通过它，我们可以把一个文件描述符的数组发给操作系统， 让操作系统去遍历，确定哪个文件描述符可以读写， 然后告诉我们去处理：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-04.gif)  
<center>select函数的大致过程</center>  
这里可以看一下select的大致伪代码，通过三个线程来进行无休止的循环来实现：  
```java
//线程1的循环：不断接受客户端连接，并把 socket 文件描述符放到一个 list 里
while(1) {
  connfd = accept(listenfd);
  fcntl(connfd, F_SETFL, O_NONBLOCK);
  fdlist.add(connfd);
}

//线程2的循环：不再自己遍历，而是调用 select，将这批文件描述符 list 交给操作系统去遍历
while(1) {
  // 把一堆文件描述符 list 传给 select 函数
  // 有已就绪的文件描述符就返回，nready 表示有多少个就绪的
  nready = select(list);
  ...
}

//线程3的循环：当 select 函数返回后，用户依然需要遍历刚刚提交给操作系统的 list，只不过，操作系统会将准备就绪的文件描述符做上标识，用户层将不会再有无意义的系统调用开销
while(1) {
  nready = select(list);
  // 用户层依然要遍历，只不过少了很多无效的系统调用
  for(fd <-- fdlist) {
    if(fd != -1) {
      // 只读已就绪的文件描述符
      read(fd, buf);
      // 总共只有 nready 个已就绪描述符，不用过多遍历
      if(--nready == 0) break;
    }
  }
}
```

总结了一下select函数的一些细节：  
- select 调用需要传入 fd 数组，需要拷贝一份到内核，高并发场景下这样的拷贝消耗的资源是惊人的。（可优化为不复制）  
- select 在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销。（内核层可优化为异步事件通知）  
- select 仅仅返回可读文件描述符的个数，具体哪个可读还是要用户自己遍历。比如加入10000个连接中，真正在发生IO的就一两个，那么剩下的也要被遍历然后调用read函数，这岂不是浪费了？（可优化为只返回给用户就绪的文件描述符，无需用户做无效的遍历）  

##### IO多路复用-poll
**它和 select 的主要区别就是，去掉了 select 只能监听 1024 个文件描述符的限制**

##### IO多路复用-epoll
epoll主要就是针对select的三个问题进行了改进：  
- 内核中保存一份文件描述符集合，无需用户每次都重新传入，只需告诉内核修改的部分即可  
- 内核不再通过轮询的方式找到就绪的文件描述符，而是通过异步 IO 事件唤醒  
- 内核仅会将有 IO 事件的文件描述符返回给用户，用户也无需遍历整个文件描述符集合  

核心在于这三个函数：`epoll_create`、`epoll_ctl`（向内核添加、修改或删除要监控的文件描述符）、`epoll_wait`（类似发起了 select() 调用）：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-05.gif)  
<center>epoll模型的大致过程</center>  

##### 总结
- 一切的开始，都起源于这个 read 函数是操作系统提供的，而且是阻塞的，这是传统阻塞IO  
- 为了破这个局，程序员在上层应用通过多线程来防止主线程卡死  
- 后来操作系统发现这个需求比较大，于是在操作系统层面提供了非阻塞的 read 函数，这样程序员就可以在一个while线程内完成多个文件描述符的读取（没有就直接快速返回-1），这就是 非阻塞 IO  
- 但多个文件描述符的读取就需要遍历，当高并发场景越来越多时，用户态遍历的文件描述符也越来越多，相当于在 while 循环里进行了越来越多的系统调用，后来操作系统又发现这个场景需求量较大，于是又在操作系统层面提供了这样的遍历文件描述符的机制，这就是 IO 多路复用  
- 多路复用有三个函数，最开始是 select，然后又发明了 poll 解决了 select 文件描述符的限制，然后又发明了 epoll 解决 select 的三个不足

引用闪客sun大佬的一段总结：  
>所以，IO 模型的演进，其实就是时代的变化，倒逼着操作系统将更多的功能加到自己的内核而已。如果你建立了这样的思维，很容易发现网上的一些错误。比如好多文章说，多路复用之所以效率高，是因为用一个线程就可以监控多个文件描述符。这显然是知其然而不知其所以然，多路复用产生的效果，完全可以由用户态去遍历文件描述符并调用其非阻塞的 read 函数实现。而多路复用快的原因在于，操作系统提供了这样的系统调用，使得原来的 while 循环里多次系统调用，变成了一次系统调用 + 内核层遍历这些文件描述符。就好比我们平时写业务代码，把原来 while 循环里调 http 接口进行批量，改成了让对方提供一个批量添加的 http 接口，然后我们一次 rpc 请求就完成了批量添加一个道理。

### Redis的分布式锁怎么玩儿？
使用Redis做分布式锁的思路大概是这样的：在redis中设置一个值表示加了锁，然后释放锁的时候就把这个key删除
##### 客户端需要做的事

```txt
// 获取锁
// NX是指如果key不存在就成功，key存在返回false，PX可以指定过期时间
SET anyLock unique_value NX PX 30000

// 释放锁：通过执行一段lua脚本
// 释放锁涉及到两条指令，这两条指令不是原子性的
// 需要用到redis的lua脚本支持特性，redis执行lua脚本是原子性的
if redis.call("get",KEYS[1]) == ARGV[1] then
return redis.call("del",KEYS[1])
else
return 0
end
```

- 一定要用`SET key value NX PX milliseconds` 命令  
    - 操作可以看成`setnx`和`expire`的结合体，是原子性的  
    - 如果不用，先设置了值，再设置过期时间，这个不是原子性操作，有可能在设置过期时间之前宕机，会造成死锁(key永久存在)  
    - 建议设置过期时间时附件随机数，避免大量的key同时过期  
- value要具有唯一性  
    - 这个是为了避免锁自动过期后被意外删除：假设A获取了锁，过期时间30s，此时35s之后，锁已经自动释放了，A去释放锁，但是此时可能B获取了锁，A客户端就不能删除B的锁了  
    - 解决办法：在解锁的时候，需要验证value是和加锁的一致才删除key
    
##### Redis服务端需要做的事
- redis有3种部署方式  
    - 单机模式  
    如果采用单机部署模式，会存在单点问题，只要redis故障了，分布式锁就失效了，所以一般单节点很少用来做分布式锁  
    - master-slave + sentinel选举模式  
    采用master-slave模式，加锁的时候只对一个节点加锁，即便通过sentinel做了高可用，但是如果master节点故障了，发生主从切换，此时就会有可能出现锁丢失的问题  
    - redis cluster模式  
- **RedLock算法**
为了解决以上的问题，Redis作者提出了一个`RedLock的算法`，这个算法的大概原理：假设redis的部署模式是redis cluster，总共有5个master节点，通过以下步骤获取一把锁：  
1. 获取当前时间戳，单位是毫秒  
2. 轮流用相同的key和随机值在这5个节点上请求锁，在这一步里，客户端在每个master上请求锁时，会有一个和总的锁释放时间相比小的多的超时时间。比如如果锁自动释放时间是10秒钟，那每个节点锁请求的超时时间可能是5-50毫秒的范围，这个可以防止一个客户端在某个宕掉的master节点上阻塞过长时间，如果一个master节点不可用了，应该尽快尝试下一个master节点  
3. 客户端计算第二步中获取锁所花的时间，只有当客户端在大多数master节点（n / 2 +1，所以在这里是3个）上成功获取了锁，而且总共消耗的时间不超过锁释放时间，这个锁就认为是获取成功了  
4. 如果锁获取成功了，那现在锁自动释放时间就是最初的锁释放时间减去之前获取锁所消耗的时间  
5. 如果锁获取失败了，不管是因为获取成功的锁不超过一半（N/2+1)还是因为总消耗时间超过了锁释放时间，客户端都会到每个master节点上释放锁

但是这样的这种算法还是颇具争议的，可能还会存在不少的问题，无法保证加锁的过程一定正确

- Redisson 实现方式（RedLock）  
Redisson是一个企业级的开源Redis Client，也提供了分布式锁的支持,只需要通过它的api中的`lock`和`unlock`即可完成分布式锁：  
    - redisson所有指令都通过lua脚本执行，redis支持lua脚本原子性执行  
    - redisson中有watchdog，它会在获取锁之后，每隔10秒去检测锁是否已经被释放了，如果没有则重置key的超时时间为最初lua脚本设定的值，这样避免了死锁  
    - 默认情况下，看门狗的检查锁的超时时间是30秒钟，也可以通过修改`Config.lockWatchdogTimeout`来另行指定
    
```xml
<dependency>
   <groupId>org.redisson</groupId>
   <artifactId>redisson</artifactId>
</dependency>  
```

```java
@Bean
public RedissonClient redissonClient() {
    Config config = new Config();
    config.useClusterServers()
            .setScanInterval(2000) // 集群状态扫描间隔时间，单位是毫秒
            //可以用"rediss://"来启用SSL连接
            .addNodeAddress("redis://127.0.0.1:7000", "redis://127.0.0.1:7001")
            .addNodeAddress("redis://127.0.0.1:7002");
    return Redisson.create(config);
}

RLock lock1 = redissonClient1.getLock("lock1");
RLock lock2 = redissonClient2.getLock("lock2");
RLock lock3 = redissonClient3.getLock("lock3");
RedissonRedLock lock = new RedissonRedLock(lock1, lock2, lock3);
// 给lock1，lock2，lock3加锁，如果没有手动解开的话，10秒钟后将会自动解开
lock.lock(10, TimeUnit.SECONDS);

// 同时加锁：lock1 lock2 lock3
// 方法一：红锁在大部分节点上加锁成功就算成功。
lock.lock();
// 方法二：为加锁等待100秒时间，并在加锁成功10秒钟后自动解开
boolean res = lock.tryLock(100, 10, TimeUnit.SECONDS);
//doSomeThing
lock.unlock();
```

### zookeeper的分布式锁
既然都说到了redis的分布式锁，那么就顺带把Zookeeper的分布式锁给总结了吧。常见的分布式锁实现方案里面，除了使用redis来实现之外，使用zookeeper也可以实现分布式锁。
##### Zookeeper是什么？
Zookeeper是一种提供配置管理、分布式协同以及命名的中心化服务。zk的模型是这样的：zk包含一系列的节点，叫做znode，就好像文件系统一样每个znode表示一个目录，然后znode有一些特性：  
- 有序节点  
假如当前有一个父节点为`/lock`，同时在这个父节点下面创建了有序的子节点（zookeeper提供了一个可选的有序特性）例如创建子节点`/lock/node-`并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号。如果是第一个创建的子节点，那么生成的子节点为`/lock/node-0000000000`，下一个节点则为`/lock/node-0000000001`，依次类推  
- 临时节点  
客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点  
- 事件监听  
在读取数据时，可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。当前zookeeper有如下四种事件：**节点创建、节点删除、节点数据修改、子节点变更**

##### Zookeeper大致原理
- 使用zk的临时节点和有序节点，每个线程获取锁就是在zk创建一个临时有序的节点，比如在/lock/目录下  
- 创建节点成功后，获取/lock目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点：如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功；如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听  
比如当前线程获取到的节点序号为`/lock/003`,然后所有的节点列表为`[/lock/001,/lock/002,/lock/003]`,则对`/lock/002`这个节点添加一个事件监听器。如果锁释放了，会唤醒下一个序号的节点，然后重新执行第3步，判断是否自己的节点序号是最小。比如`/lock/001`释放了，`/lock/002`监听到事件，此时节点集合为`[/lock/002,/lock/003]`,则`/lock/002`为最小序号节点，获取到锁  

##### 怎么玩？
Curator是一个zookeeper的开源客户端，也提供了分布式锁的实现：  
![](https://gitee.com/liaoxinyiqiqi/my-blog-images/raw/master/img/java-redis-02-02.jpg)  
<center>curator实现分布式锁的底层原理</center>

代码如下：

```java
InterProcessMutex interProcessMutex = new InterProcessMutex(client,"/anyLock");
interProcessMutex.acquire();
interProcessMutex.release();
```

##### 总结
对于redis的分布式锁而言，它有以下缺点：  
- 它获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能  
- 另外来说的话，redis的设计定位决定了它的数据并不是强一致性的，在某些极端情况下，可能会出现问题。锁的模型不够健壮，即便使用redlock算法来实现，在某些复杂场景下，也无法保证其实现100%没有问题，关于redlock的讨论可以看How to do distributed locking  
- 但是另一方面使用redis实现分布式锁在很多企业中非常常见，而且大部分情况下都不会遇到所谓的极端复杂场景，所以使用redis作为分布式锁也不失为一种好的方案，最重要的一点是redis的性能很高，可以支撑高并发的获取、释放锁操作  

对于zk分布式锁而言:
- zookeeper天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁  
- 如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小  
- 但是如果有较多的客户端频繁的申请加锁、释放锁，对于zk集群的压力会比较大

怎么选用要看具体的实际场景了，如果公司里面有zk集群条件，优先选用zk实现，但是如果说公司里面只有redis集群，没有条件搭建zk集群。那么其实用redis来实现也可以，另外还可能是系统设计者考虑到了系统已经有redis，但是又不希望再次引入一些外部依赖的情况下，可以选用redis  